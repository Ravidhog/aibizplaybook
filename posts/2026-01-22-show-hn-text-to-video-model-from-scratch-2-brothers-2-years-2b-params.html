<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)</title>
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <header>
    <h1>Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)</h1>
    <p><small>Published: 2026-01-22T16:31:47+00:00</small></p>
  </header>

  <main>
    <p>Writeup (includes good/bad sample generations): <a href="https://www.linum.ai/field-notes/launch-linum-v2">https://www.linum.ai/field-notes/launch-linum-v2</a><p>We're Sahil and Manu, two brothers who spent the last 2 years training text-to-video models from scratch. Today we're releasing them under Apache 2.0.<p>These are 2B param models capable of generating 2-5 seconds of footage at either 360p or 720p. In terms of model size, the closest comparison is Alibaba's Wan 2.1 1.3B. From our testing, we get significantly better motion capture and aesthetics.<p>We're not claiming to have reached the frontier. For us, this is a stepping stone towards SOTA - proof we can train these models end-to-end ourselves.<p>Why train a model from scratch?<p>We shipped our first model in January 2024 (pre-Sora) as a 180p, 1-second GIF bot, bootstrapped off Stable Diffusion XL. Image VAEs don't understand temporal coherence, and without the original training data, you can't smoothly transition between image and video distributions. At some point you're better off starting over.<p>For v2, we use T5 for text encoding, Wan 2.1 VAE for compression, and a DiT-variant backbone trained with flow matching. We built our own temporal VAE but Wan's was smaller with equivalent performance, so we used it to save on embedding costs. (We'll open-source our VAE shortly.)<p>The bulk of development time went into building curation pipelines that actually work (e.g., hand-labeling aesthetic properties and fine-tuning VLMs to filter at scale).<p>What works: Cartoon/animated styles, food and nature scenes, simple character motion. What doesn't: Complex physics, fast motion (e.g., gymnastics, dancing), consistent text.<p>Why build this when Veo/Sora exist? Products are extensions of the underlying model's capabilities. If users want a feature the model doesn't support (character consistency, camera controls, editing, style mapping, etc.), you're stuck. To build the product we want, we need to update the model itself. That means owning the development process. It's a bet that will take time (and a lot of GPU compute) to pay off, but we think it's the right one.<p>What’s next?
- Post-training for physics/deformations
- Distillation for speed
- Audio capabilities
- Model scaling<p>We kept a “lab notebook” of all our experiments in Notion. Happy to answer questions about building a model from 0 → 1.  Comments and feedback welcome!</p>
<hr />
<p>Comments URL: <a href="https://news.ycombinator.com/item?id=46721488">https://news.ycombinator.com/item?id=46721488</a></p>
<p>Points: 70</p>
<p># Comments: 13</p>
    <p><a href="https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video" rel="noopener noreferrer">Source</a></p>
  </main>

  <aside>
    <!-- Paste AdSense snippet here -->

  </aside>

  <script src="/site.js"></script>
</body>
</html>
